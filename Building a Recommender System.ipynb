{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Recommender System\n",
    "\n",
    "In this notebook we will build a recommender system training embeddings on a subset of the data we extracted in the other notebook. This data consists of a subset of 10k movies. You can find it in `data/wp_movies_10k.ndjson`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before starting...\n",
    "\n",
    "We need to import the libaries we'll use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training movie embeddings\n",
    "\n",
    "We want to use our link data between entities to recommend content. \n",
    "\n",
    "We'll achieve this by training embeddings using connections drawn from some metainformation about the outgoing links of each movie. Why? Because they might share the same director, staff, actors, or have been released close to each other. In general, links from a movie article to another shows a certain relationship we want to exploit.\n",
    "\n",
    "Let's start by counting the outgoing links as a quick way to see if what we have is reasonable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Rotten Tomatoes', 9393), ('Category:English-language films', 5882), ('Category:American films', 5867), ('Variety (magazine)', 5450), ('Metacritic', 5112), ('Box Office Mojo', 4186), ('The New York Times', 3818), ('The Hollywood Reporter', 3553), ('Roger Ebert', 2707), ('Los Angeles Times', 2454)]\n"
     ]
    }
   ],
   "source": [
    "with open('data/wp_movies_10k.ndjson') as f:\n",
    "    movies = [json.loads(line) for line in f]\n",
    "\n",
    "link_counts = Counter()\n",
    "\n",
    "for movie in movies:\n",
    "    movie_links = movie[2]\n",
    "    link_counts.update(movie_links)\n",
    "\n",
    "print(link_counts.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our task is to determine wether a certain link can be found on the Wikipedia page of a movie. Hence, we need to feed it a proper dataset of matches vs non-matches. For this, we'll preserve only links that occur at least three times.\n",
    "\n",
    "We will also build a list of valid (link, movie) pairs that'll speed up our lookups in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pairs: 949544\n",
      "Number of links: 66913\n",
      "Number of movies: 10000\n"
     ]
    }
   ],
   "source": [
    "top_links = [link for link, count in link_counts.items() if count >= 3]\n",
    "link_to_index = {link: index for index, link in enumerate(top_links)}\n",
    "movie_to_index = {movie[0]: index for index, movie in enumerate(movies)}\n",
    "\n",
    "pairs = []\n",
    "for movie in movies:\n",
    "    movie_title = movie[0]\n",
    "    movie_links = movie[2]\n",
    "    pairs.extend((link_to_index[link], movie_to_index[movie_title])\n",
    "                 for link in movie_links\n",
    "                 if link in link_to_index)\n",
    "\n",
    "pairs_set = set(pairs)\n",
    "print(f'Number of pairs: {len(pairs)}')\n",
    "print(f'Number of links: {len(top_links)}')\n",
    "print(f'Number of movies: {len(movie_to_index)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. We can now move on to building our embeddings. We'll use Keras for this purpose.\n",
    "\n",
    "The way this model will work is by taking both the link_id and movie_id, feeding them to the corresponding embedding layers, which then will allocate a vector of `embedding_size` for each possible input. Afterwards, the output of the model will be the dot product of both vectors. What'll happen is that the model will learn weights such that this dot product is as close to the actual label as possible. \n",
    "\n",
    "These weights will then act as projectors of movies and links to a multidimensional space where similar movies end up close to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def get_movie_embedding_model(top_links, movie_to_index, embedding_size=50):\n",
      "    link = Input(name='link', shape=(1,))\n",
      "    movie = Input(name='movie', shape=(1,))\n",
      "\n",
      "    link_embedding = Embedding(name='link_embedding',\n",
      "                               input_dim=len(top_links),\n",
      "                               output_dim=embedding_size)(link)\n",
      "    movie_embedding = Embedding(name='movie_embedding',\n",
      "                                input_dim=len(movie_to_index),\n",
      "                                output_dim=embedding_size)(movie)\n",
      "    dot = Dot(name='dot_product',\n",
      "              normalize=True,\n",
      "              axes=2)([link_embedding, movie_embedding])\n",
      "\n",
      "    merged = Reshape(target_shape=(1,))(dot)\n",
      "\n",
      "    model = Model(inputs=[link, movie], outputs=[merged])\n",
      "    model.compile(optimizer='nadam', loss='mse')\n",
      "\n",
      "    return model\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "link (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "movie (InputLayer)              (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "link_embedding (Embedding)      (None, 1, 50)        3345650     link[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "movie_embedding (Embedding)     (None, 1, 50)        500000      movie[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_product (Dot)               (None, 1, 1)         0           link_embedding[0][0]             \n",
      "                                                                 movie_embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1)            0           dot_product[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 3,845,650\n",
      "Trainable params: 3,845,650\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(get_movie_embedding_model))\n",
    "\n",
    "model = get_movie_embedding_model(top_links, movie_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, we will use a generator that yields batches of data made up of possitive and negative examples.\n",
    "\n",
    "The positives instances are sampled from the `pairs` list and then fill it up with negative examples, which are randomly picked (we double check they're not in the `pairs_set`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def batchifier(pairs, pairs_set, top_links, movie_to_index, positive_samples=50, negative_ratio=10):\n",
      "    batch_size = positive_samples * (1 + negative_ratio)\n",
      "    batch = np.zeros((batch_size, 3))\n",
      "\n",
      "    while True:\n",
      "        for index, (link_id, movie_id) in enumerate(random.sample(pairs, positive_samples)):\n",
      "            batch[index, :] = (link_id, movie_id, 1)\n",
      "\n",
      "        index = positive_samples\n",
      "\n",
      "        while index < batch_size:\n",
      "            movie_id = random.randrange(len(movie_to_index))\n",
      "            link_id = random.randrange(len(top_links))\n",
      "\n",
      "            if not (link_id, movie_id) in pairs_set:\n",
      "                batch[index, :] = (link_id, movie_id, -1)\n",
      "                index += 1\n",
      "\n",
      "        np.random.shuffle(batch)\n",
      "\n",
      "        yield {'link': batch[:, 0],\n",
      "               'movie': batch[:, 1]}, batch[:, 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(batchifier))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1854/1854 [==============================] - 202s 109ms/step - loss: 0.3771\n",
      "Epoch 2/15\n",
      "1854/1854 [==============================] - 199s 107ms/step - loss: 0.2314\n",
      "Epoch 3/15\n",
      "1854/1854 [==============================] - 199s 108ms/step - loss: 0.2261\n",
      "Epoch 4/15\n",
      "1854/1854 [==============================] - 199s 108ms/step - loss: 0.2255\n",
      "Epoch 5/15\n",
      "1854/1854 [==============================] - 199s 107ms/step - loss: 0.2223\n",
      "Epoch 6/15\n",
      "1854/1854 [==============================] - 199s 107ms/step - loss: 0.2217\n",
      "Epoch 7/15\n",
      "1854/1854 [==============================] - 199s 107ms/step - loss: 0.2206\n",
      "Epoch 8/15\n",
      "1854/1854 [==============================] - 199s 107ms/step - loss: 0.2192\n",
      "Epoch 9/15\n",
      "1854/1854 [==============================] - 199s 108ms/step - loss: 0.2189\n",
      "Epoch 10/15\n",
      "1854/1854 [==============================] - 199s 108ms/step - loss: 0.2194\n",
      "Epoch 11/15\n",
      "1854/1854 [==============================] - 199s 107ms/step - loss: 0.2178\n",
      "Epoch 12/15\n",
      "1854/1854 [==============================] - 199s 107ms/step - loss: 0.2176\n",
      "Epoch 13/15\n",
      "1854/1854 [==============================] - 199s 107ms/step - loss: 0.2165\n",
      "Epoch 14/15\n",
      "1854/1854 [==============================] - 199s 107ms/step - loss: 0.2186\n",
      "Epoch 15/15\n",
      "1854/1854 [==============================] - 199s 107ms/step - loss: 0.2184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f64869fbbe0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(5)\n",
    "\n",
    "positive_samples_per_batch = 512\n",
    "data_generator = batchifier(pairs, pairs_set, top_links, movie_to_index, positive_samples_per_batch)\n",
    "steps_per_epoch = len(pairs) // positive_samples_per_batch\n",
    "\n",
    "model.fit_generator(data_generator,\n",
    "                    epochs=15,\n",
    "                    steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the movie and link embeddings from the model accessing the corresponding layer by their name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_embedding = model.get_layer('movie_embedding')\n",
    "movie_weights = movie_embedding.get_weights()[0]\n",
    "movie_lengths = np.linalg.norm(movie_weights, axis=1)\n",
    "normalized_movies = (movie_weights.T / movie_lengths).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the embeddings make sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def similar_movies(movie, movies, normalized_movies, movie_to_index, top_n=10):\n",
      "    distances = np.dot(normalized_movies, normalized_movies[movie_to_index[movie]])\n",
      "    closest = np.argsort(distances)[-top_n:]\n",
      "\n",
      "    for c in reversed(closest):\n",
      "        movie_title = movies[c][0]\n",
      "        distance = distances[c]\n",
      "        print(c, movie_title, distance)\n",
      "\n",
      "29 Rogue One 0.99999994\n",
      "19 Interstellar (film) 0.9814422\n",
      "3349 Star Wars: The Force Awakens 0.9689913\n",
      "25 Star Wars sequel trilogy 0.9672627\n",
      "659 Rise of the Planet of the Apes 0.96509415\n",
      "245 Gravity (film) 0.9645235\n",
      "86 Tomorrowland (film) 0.9591576\n",
      "372 The Amazing Spider-Man (2012 film) 0.9586833\n",
      "181 Pacific Rim (film) 0.9578317\n",
      "37 Avatar (2009 film) 0.9577962\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(similar_movies))\n",
    "\n",
    "similar_movies('Rogue One', movies, normalized_movies, movie_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same deal with links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def similar_links(link, top_links, normalized_links, link_to_index, top_n=10):\n",
      "    distances = np.dot(normalized_links, normalized_links[link_to_index[link]])\n",
      "    closest = np.argsort(distances)[-top_n:]\n",
      "    \n",
      "    for l in reversed(closest):\n",
      "        distance = distances[l]\n",
      "        print(l, top_links[l], distance)\n",
      "\n",
      "127 George Lucas 0.99999994\n",
      "2707 Star Wars 0.93741417\n",
      "4830 widescreen 0.93099356\n",
      "3176 Star Wars (film) 0.9273331\n",
      "976 Hugo Award for Best Dramatic Presentation 0.91403615\n",
      "2931 LaserDisc 0.89742184\n",
      "2829 storyboard 0.8899722\n",
      "2860 Steven Spielberg 0.8812173\n",
      "4051 novelization 0.88026863\n",
      "1732 Academy Award for Best Visual Effects 0.8759304\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(similar_links))\n",
    "    \n",
    "link_embedding = model.get_layer('link_embedding')\n",
    "link_weights = link_embedding.get_weights()[0]\n",
    "link_lengths = np.linalg.norm(link_weights, axis=1)\n",
    "normalized_links = (link_weights.T / link_lengths).T\n",
    "\n",
    "similar_links('George Lucas', top_links, normalized_links, link_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Movie Recommender\n",
    "\n",
    "With our embeddings properly trained and working, we can use them to train a simple classifier, such as an SVM to separate positively ranked items from negative.\n",
    "\n",
    "Given we don't have any users, we cannot use user data to train the classifier, so we need to fake it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 50)\n"
     ]
    }
   ],
   "source": [
    "best = ['Star Wars: The Force Awakens', 'The Martian (film)', 'Tangerine (film)', 'Straight Outta Compton (film)',\n",
    "        'Brooklyn (film)', 'Carol (film)', 'Spotlight (film)']\n",
    "worst = ['American Ultra', 'The Cobbler (2014 film)', 'Entourage (film)', 'Fantastic Four (2015 film)',\n",
    "         'Get Hard', 'Hot Pursuit (2015 film)', 'Mortdecai (film)', 'Serena (2014 film)', 'Vacation (2015 film)']\n",
    "\n",
    "all_data = best + worst\n",
    "y = np.asarray(([1] * len(best)) + ([0] * len(worst)))\n",
    "X = np.asarray([normalized_movies[movie_to_index[movie]]\n",
    "                for movie in all_data])\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training an SVM on this data is so easy that it feels like cheating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = svm.SVC(kernel='linear')\n",
    "classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the classifier over all of our movies and pick the top 5 best and top 5 worst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: \n",
      "66 Skyfall 1.3088888053043541\n",
      "481 The Devil Wears Prada (film) 1.3019425459507856\n",
      "458 Hugo (film) 1.1653304719497952\n",
      "307 Les Mis√©rables (2012 film) 1.1385154200244971\n",
      "3 Spectre (2015 film) 1.056846426209078\n",
      "Worst: \n",
      "5097 Ready to Rumble -1.5441925266385934\n",
      "9595 Speed Zone -1.527474667206325\n",
      "1878 The Little Rascals (film) -1.4972133841701225\n",
      "8559 Air Buddies -1.4825302881861921\n",
      "7593 Trojan War (film) -1.4619956658429054\n"
     ]
    }
   ],
   "source": [
    "estimated_movie_ratings = classifier.decision_function(normalized_movies)\n",
    "    \n",
    "best = np.argsort(estimated_movie_ratings)\n",
    "print('Best: ')\n",
    "for movie_index in reversed(best[-5:]):\n",
    "    movie_title = movies[movie_index][0]\n",
    "    movie_rating = estimated_movie_ratings[movie_index]\n",
    "    print(movie_index, movie_title, movie_rating)\n",
    "\n",
    "print('Worst: ')\n",
    "for movie_index in best[:5]:\n",
    "    movie_title = movies[movie_index][0]\n",
    "    movie_rating = estimated_movie_ratings[movie_index]\n",
    "    print(movie_index, movie_title, movie_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Simple Movie Properties\n",
    "\n",
    "We can also use our embeddings to predict simple stuff about movies, like Rotten Tomatoes ratings. Let's do that!\n",
    "\n",
    "For this task we'll resort to Linear Regression.\n",
    "\n",
    "The Rotten Tomatoes score of a movie is in `movie[-2]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotten tomatoes score.\n",
    "rotten_y = np.asarray([float(movie[-2][:-1]) / 100 for movie in movies if movie[-2]])\n",
    "# Vectors representing movie titles.\n",
    "rotten_X = np.asarray([normalized_movies[movie_to_index[movie[0]]] for movie in movies if movie[-2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error using linear regression: 0.06131464247800219\n",
      "Mean Squared Error using just the mean: 0.08957773784832902\n"
     ]
    }
   ],
   "source": [
    "TRAINING_SIZE = 0.8\n",
    "SPLIT_POINT = int(len(rotten_X) * TRAINING_SIZE)\n",
    "rotten_X_train = rotten_X[:SPLIT_POINT]\n",
    "rotten_y_train = rotten_y[:SPLIT_POINT]\n",
    "\n",
    "rotten_X_test = rotten_X[SPLIT_POINT:]\n",
    "rotten_y_test = rotten_y[SPLIT_POINT:]\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(rotten_X_train, rotten_y_train)\n",
    "\n",
    "error = regressor.predict(rotten_X_test) - rotten_y_test\n",
    "print(f'Mean Squared Error using linear regression: {np.mean(error ** 2)}')\n",
    "\n",
    "training_rotten_tomatoes_mean_score = np.mean(rotten_y_train)\n",
    "error = training_rotten_tomatoes_mean_score - rotten_y_test\n",
    "print(f'Mean Squared Error using just the mean: {np.mean(error ** 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! It seems our little regressor does well. But there's a catch: Given we used only the top 10k movies, their Rotten Tomatoes score is fairly similar and tend to be good. Hence, just predicting the mean won't give bad results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
